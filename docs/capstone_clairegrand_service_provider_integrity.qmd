---
title: "Integrity practices of global water service providers"
subtitle: "A review of InWASH integrity assessment data"
author: "Claire Grandadam"
affiliation: "Water Integrity Network"
date: "January 2026"
format:
  html:
    embed-resources: true
    toc: true
    toc-depth: 1
    toc-title: "Contents"
execute:
  warning: false
editor_options:
  chunk_output_type: console
editor: 
  markdown: 
    wrap: 72
---

# Introduction

To reach national policy objectives for water and sanitation service
delivery and SDG6, water service providers globally must increase
service coverage and service quality in the face of growing challenges
related to financing, population growth, and climate change. This has
spurred high interest in understanding the drivers of service provider
performance, particularly from service provider managers, policy-makers,
regulators, banks and other funders, as well as sector analysts.

National and global utility benchmarking processes have been used widely
for such analysis, to assess the way service providers operate, and to
orient sector reforms. The most common benchmarking tools tend to focus
on service quality, financial management, and operational efficiency.
Recently, water sector utility benchmarking tools have also been
evolving in part in response to two developments:

-   First, the recognition that performance is affected by governance
    issues and corruption, and the related evidence that anti-corruption
    and integrity management can support utility performance
    (Barreto-Dillon, et al., 2018; WIN, 2021; IDB, 2024). There is a
    need for better tools that can support service providers striving
    for integrity, equity, sustainability, and resilience.

-   Second, growing criticism of the limits and bias in major utility
    benchmarking tools that have primarily focused on efficiency, to the
    detriment of distributional equity (Bhatt, 2024).

Major utility benchmarking tools such as Aquarating and NewIBnet
recently introduced new process indicators focusing on management
practices (in 2022 and 2023 respectively). There is however limited data
available on their use. The Water Integrity Network (WIN) also developed
a set of indicators to assess integrity management practices of service
providers.

This analysis aims to examine anonymised data from applications of the
Water Integrity Network tool since 2018, to understand patterns of use,
possible resistance and challenges to assessing integrity practices, and
trends in integrity management practices.

# Methods

WIN developed integrity indicators for utilities from 2018 onwards with
support from the Inter-American Development Bank and in partnership with
the Consortium for Water Integrity in Latin America (with SIWI and
cewas). This set of indicators is sometimes referred to as the Utility
Integrity Assessment. It is part of WIN's InWASH integrity management
toolbox for water and sanitation service providers, an intensive process
for prioritising and managing the integrity risks service providers face
in their daily operations. The tool is also available separately as an
online survey at <https://www.waterintegritynetwork.net/inwash>.

Local context and the local regulatory environment highly influence
which governance and integrity practices are possible and effective for
different utilities. Still, early pilots and assessment by WIN indicated
that there are patterns in how good governance and integrity practices
are implemented in organisations in the water and sanitation sectors
globally and that benchmarking can be useful for urban service providers
of a certain size (very small rural service providers tend to not yet
have the capacity and systems for the governance practices described in
the tool). The tool is based on these observations.

The Utility Integrity Assessment was designed as a short survey
examining 15 indicators categorised under 5 integrity principles:

1.  Tone at the top (2 indicators: on leadership and on codes of
    conduct);

2.  Integrity risk assessment (1 indicator);

3.  Integrity controls (7 indicators, on conflicts of interest,
    whistleblowing, recruitment, procurement, disclosure, participation,
    and financial management);

4.  Corrective action (2 indicators, on sanctions for staff and on
    sanctions for contractors);

5.  Monitoring (3 indicators, on risk monitoring, review of risk
    management functions, and responsiveness to external accountability
    mechanisms).

Service providers use the tool for a self-assessment. They score
themselves for each indicator on a scale of 1 to 5, based on clear
descriptions of the practices that they would have in place for each
score. For example, the second indicator (IRM1.2) examines how
effectively a service provider sets integrity standards and enforces
them through a code of conduct or similar document. The 5 possible
scores are:

-   **Score 1:** There is no written code of conduct (or similar
    document) outlining what the Utility expects from staff regarding
    values, rules, standards, and principles.

-   **Score 2:** Between 1 and 3.

-   **Score 3:** The code of conduct (or a similar document) contains
    most of the following elements (some elements may be missing): an
    ethical framework for decision making, generic examples of what
    constitute acceptable and unacceptable behaviour, guidelines on
    reporting problems anonymously, accountability and disciplinary
    policies for unethical behaviour, a list of ethics and compliance
    resources. The code of conduct was not revised in the last 3
    calendar years. There is evidence that the Utility has organised a
    training on the code of conduct in the past, but the training is not
    routinely provided.

-   **Score 4:** Between 3 and 5.

-   **Score 5:** The code of conduct (or a similar document) contains
    ALL of the elements outlined in Level 3. The code of conduct was
    revised at most 3 years ago or more recently, after 1) the most
    recent changes to workplace profiles, including restructuring,
    relocation, changes in key roles or decision-making processes, or 2)
    the most recent changes in the external environment, including
    sector reform, new relevant legislation, changes in government
    strategies or in contractorsâ€™ business practices. Code of conduct
    training is routinely given to new employees as part of their
    induction programme.

This analysis looks at available data since 2018, from service providers
that filled the survey online independently and from service providers
that participated in an InWASH process and received support and
validation on the Utility Integrity Assessment from a WIN or partner
expert or consultant.

The analysis looks at:

-   whether scores differ based on the type of utility, the way the the
    survey was taken, or within utilities (over time or between
    individual survey takers).

-   whether there is significant variation in the practices of service
    providers across regions,

-   whether there are clear trends, strengths, and weaknesses across
    indicators and principles,

```{r  echo = TRUE}

library(tidyverse)
library(gt)
library(gtsummary)
library(knitr)
library(DT)

ia_data <- read_csv(here::here("data/processed/processed_data.csv"))

glimpse(ia_data)

tbl_data_overview <- ia_data |>  
  summarise(
      online_count = sum(on_off == "online", na.rm = TRUE),
      offline_count = sum(on_off == "offline", na.rm = TRUE), 
      completed = sum(last_page > 4),
      incomplete = sum(last_page < 5),
      anon = sum(region == "UNKNOWN"),
      utilities = n_distinct(utility_code,  na.rm = TRUE),
      countries = n_distinct(country_id,  na.rm = TRUE)
  )

```

@tbl-data-overview shows the number of entries assessed, noting how many
were submitted online and offline, how many were complete or not, how
many were anonymous, and the number of utilities and countries
represented.

```{r}
#| label: tbl-data-overview
#| tbl-cap: "Overview of utility integrity assessment data"

tbl_data_overview |>
    gt() |>
    tab_header(title = "Utility integrity assessment",
              subtitle = "Data from 75 surveys") |>
    cols_label(online_count = "Submitted online",
               offline_count = "Submitted offline",
               completed = "Complete surveys",
               incomplete = "Incomplete surveys",
               anon = "Anonymous entries",
               utilities = "Distinct utilities",
               countries = "Distinct countries")
```

# Results

## Survey completeness

```{r  echo = TRUE}

library(ggplot2)
library(ggthemes)
library(ggpattern)

ia_data <- ia_data |> mutate(status = case_when(
    last_page > 4 ~ "complete",
    last_page < 5 ~ "incomplete"
    ))
```

Most respondents were able to complete the survey (over 70%). This could
indicate that there are still categories of service provider for whom
the indicators are not able to easy to follow, relevant, or applicable
to their context. @fig-completeness shows that all respondents using the
offline tool with support completed the survey in full and provided
contact information on page 6. Most online respondents also completed
the survey, even without dedicated support, though 12 did abandon early
on, providing information for only up to 3 indicators out of 15, on page
1 (indicator IRM1.1 and IRM1.2) and page 2 (IRM2.1).

```{r}
#| label: fig-completeness
#| fig-cap: "Integrity assessment completeness by input method (online/offline)"

ggplot(ia_data, aes(
    x = last_page,
    fill = on_off)) +
  geom_bar_pattern(aes(pattern_fill = status),
                   pattern = 'circle') +
    labs(title = "Integrity assessment completeness by input method (online / offline)",
       x = "Last page acted on",
       y = "Count",
       fill = "Online / Offline",
       pattern_fill = "Status") +
  scale_fill_grey(start = 0.4, end = 0.8) +
  theme_minimal()

```

## Score trends

```{r}
#|eval: true

library(ggplot2)
library(ggthemes)


ia_ordered_regions <- ia_data |> mutate(
  region = factor(region, levels = c("Eastern and Southern Africa", "Middle East and North Africa", "South Asia", "Latin America and Caribbean", "Western Europe", "UNKNOWN" ))
  )


ia_data_long <- ia_ordered_regions |>
    relocate(mean_principle1, .after = last_col()) |> 
    relocate(mean_principle2, .after = last_col()) |> 
    relocate(mean_principle3, .after = last_col()) |> 
    relocate(mean_principle4, .after = last_col()) |> 
    relocate(mean_principle5, .after = last_col()) |> 
      mutate(id = as.character(id)) |>
        pivot_longer(cols = irm11_short:irm53_short,
               names_to = "indicator",
               values_to = "score")


score_overview <- ia_data_long |> 
  group_by(indicator) |> 
  summarise(
    mean_scores = mean(score, na.rm = TRUE),
    mode_scores = names(sort(table(score), decreasing = TRUE)[1]),
    sd_scores = sd(score, na.rm = TRUE)
  )

score_overview

ia_data_long_anon <- ia_data_long |> 
   mutate(anonymity = case_when(
  region == "UNKNOWN" ~ "unknown respondent",
  TRUE ~ "known respondent"
))

score_overview_anon <- ia_data_long_anon |> 
  group_by(indicator, anonymity) |> 
  summarise(
    mean_scores = mean(score, na.rm = TRUE),
    mode_scores = names(sort(table(score), decreasing = TRUE)[1]),
    sd_scores = sd(score, na.rm = TRUE)
  )

score_overview_anon


ia_data_long_filteredanon <- ia_data_long |> 
  filter(region != "UNKNOWN")

score_overview_filtered <- ia_data_long_filteredanon |> 
  group_by(indicator) |> 
  summarise(
    mean_scores = mean(score, na.rm = TRUE),
    mode_scores = names(sort(table(score), decreasing = TRUE)[1]),
    sd_scores = sd(score, na.rm = TRUE)
  )

score_overview_filtered 

score_overview_filtered_source <- ia_data_long_filteredanon |> 
  group_by(indicator, on_off) |> 
  summarise(
    mean_scores = mean(score, na.rm = TRUE),
    mode_scores = names(sort(table(score), decreasing = TRUE)[1]),
    sd_scores = sd(score, na.rm = TRUE)
  )

score_overview_filtered_source

score_overview_filtered_regional <- ia_data_long_filteredanon |> 
  group_by(indicator, region) |> 
  summarise(
    mean_scores = mean(score, na.rm = TRUE),
    mode_scores = names(sort(table(score), decreasing = TRUE)[1]),
    sd_scores = sd(score, na.rm = TRUE)
  )

score_overview_filtered_regional
```

add scores per principle

```{r}
#| label: tbl-score-overview
#| tbl-cap: "Utility integrity assessment - Indicator scoring"

score_overview |>
    gt() |>
    tab_header(title = "Utility Integrity Assessment Scores") |>
    cols_label(indicator = "Indicator", 
               mean_scores = "Mean",
               mode_scores = "Mode",
               sd_scores = "Standard Deviation"
               ) |> 
    fmt_number(
      columns = everything(), 
      decimals = 2)
```

```{r}
#| label: fig-score-comp-anon
#| fig-cap: "Mean scores across integrity assessment indicators, with comparison of entries by known or anonymous respondents"

ggplot(score_overview, aes(
    x = indicator,
    y = mean_scores)) +
  geom_col() +
  geom_point(data = score_overview_anon, 
            aes(x = indicator, y = mean_scores, group = anonymity, color = anonymity)) +
  labs(title = "Mean scores across integrity assessment indicators, with comparison of entries by known or anonymous respondents",
       x = "Indicator",
       y = "Mean scores", 
       color = "Data anonymity") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 80, hjust = 0.8))
```

```{r}
#| label: fig-score-comp-source
#| fig-cap: "Comparison of mean scores across integrity assessment indicators from known respondents, according to source (online/offline)"

ggplot(score_overview_filtered_source, aes(
    x = indicator,
    y = mean_scores, 
    fill = on_off)) +
  geom_col(position = "dodge") +
  labs(title = "Comparison of mean scores across integrity assessment indicators from known respondents, according to source (online/offline)",
       x = "Indicator",
       y = "Mean scores", 
       fill = "Source (online/offline)") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 80, hjust = 0.8))
```

```{r}
#| label: fig-score-comp-region
#| fig-cap: "Comparison of mean scores across integrity assessment indicators from known respondents, according to region"

ggplot(score_overview_filtered, aes(
    x = indicator,
    y = mean_scores)) +
  geom_col() +
  geom_point(data = score_overview_filtered_regional, 
            aes(x = indicator, y = mean_scores, group = region, color = region)) +
  geom_line(data = score_overview_filtered_regional, 
            aes(x = indicator, y = mean_scores, group = region, color = region)) + 
  labs(title = "Comparison of mean scores across integrity assessment indicators from known respondents, according to region",
       x = "Indicator",
       y = "Mean scores", 
       color = "Region") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 80, hjust = 0.8))
```

# Conclusions

# References
